---
title: "The Leverage Swap: Manager to IC in 2026"
description: "Will AI become a purer lever?"
pubDate: 2026-01-12
status: "Sapling"
locale: "en"
tags: ["Career", "Management", "AI", "Thinking"]
draft: false
---

I am currently in a professional observation period: after a few years as a Software Engineering Manager at Google, I am seriously preparing to return to an Individual Contributor (IC) role later this year.

To be clear, this transition has not officially begun. These reflections serve as a "mental blueprint" prior to the actual move. I want to document my current logic so that once the transition is complete and I’ve been in the role for some time, I can write a "companion piece" to test these hypotheses with the benefit of hindsight.

The core of all these hypotheses points to one keyword: Leverage.

Observation & Hypothesis: Is the Old Math Still Valid?

In the classic context of management, influence is almost synonymous with the mobilization of others. In "High Output Management", Andy Grove provides a rigorous formula: A manager’s output = the output of the organization under his supervision or influence. This is a game of "scale effect." To achieve this, we must go through what "The Leadership Pipeline" calls a "shift in values": letting go of the obsession with personal output and instead achieving exponential growth through delegation and coaching, as described in "Multipliers".

However, in my daily observations, this human leverage is not a pure physical addition; it is mixed with extremely high "Communication Entropy."

A rather extreme example: I have a team member who is highly motivated and energetic but has a fatal flaw—he only "reads half the message." Whether in group chats or task descriptions, if I ask two questions, he only answers one. This forces me to follow up repeatedly or clarify again in meetings. While simple clarifications are manageable, it becomes disastrous when discussing architectural premises—if he builds something based on only half the context, the result is often misaligned at best and a complete waste of time requiring a total rewrite at worst.

Traditional management theory suggests solving this through feedback, coaching, or repositioning the individual. But all these efforts are essentially "maintenance costs" paid to coordinate human labor. I have begun to wonder: In an era of radical technological change, is the cost of maintaining "human leverage" beginning to swallow the output it generates?

Will AI Become a Purer Form of Leverage?

Based on the iterative observation of AI capabilities and following the methodology of Syntopical Reading proposed by Mortimer Adler in "How to Read a Book"—which is the core methodology of my writing series—I am preparing to test the following three key hypotheses during my transition:

1. From "Multiplying Others" to "Multiplying Self": Seeking "Escape Velocity"

In physics, Escape Velocity is a fascinating concept: only by moving fast enough can one break free from Earth's gravitational pull into deep space. I have always believed that a top-tier team needs this momentum, fueled by both "speed" and "quality." Yet, in traditional management, members who communicate poorly or provide slow feedback often act like giant parachutes attached to the back of the spacecraft, drastically slowing down the overall propulsion.

While "Multipliers" encourages managers to tap into team potential through questioning, in my hypothesis, AI is a purer, emotionless "multiplier" with zero friction.

Hypothesis: Compared to managing a team with varying performance and high communication overhead, AI orchestration can increase information transfer efficiency by several orders of magnitude. This "zero-resistance" communication allows personal output to accelerate exponentially, making it possible for an individual contribution to reach the "escape velocity" necessary to break free from the gravity of mediocrity.

2. Efficiency in Capital Allocation: From "Patching Weakness" to "Allocating Strengths"

This is a resource game involving "time"—the ultimate hard currency. As a frontline manager in a large organization, my duties require me to invest a massive "premium" of time into underperforming members because my KPIs include "Coaching" and "Talent Growth."

However, in my vision for the 2026 IC transition, I will gain the freedom of "selective collaboration."

Hypothesis: AI acts as a high-efficiency financial system, significantly lowering the barrier to "solo execution." This allows time to circulate through the system as efficiently as money. I am no longer morally obligated to repair inefficient collaboration chains. Instead, I can precisely select and enjoy the high-quality time output of others. If a collaboration fails to meet ROI expectations, I can quickly withdraw my investment and pivot to solving the problem myself via AI, ensuring my "time capital" is always allocated to the highest-yielding areas.

3. From Linear Extension to Non-Linear Innovation: The "Big Picture IC"

Traditional logic assumes managers see the big picture (Business & Program) while ICs just keep their heads down. But managers are often trapped by current resource boundaries and schedules; their logic is usually one of "linear extension"—adding a bit more resource to the existing track. An AI-augmented IC, freed from execution overhead, actually gains the "luxury time" to observe the whole.

Hypothesis: Because ICs do not have to sink into endless alignment meetings, they can use AI to rapidly scan cross-team codebases and untangle complex business chains. This combination of depth and breadth allows them to propose "non-linear" innovations. Instead of patching an old system, they can identify structural flaws and propose fundamental reconstructions. As "The Staff Engineer’s Path" suggests, the true value of a Staff Engineer lies in solving systemic deadlocks with an elegant, cross-team technical solution that a manager couldn't resolve even with ten additional hires.

Mindset Preparation

Since the transition hasn't happened yet, my current focus is on "pre-setting" my mindset. I am trying to answer these unsettling questions:

De-mystifying "Power": Can I accept losing administrative "decision-making power" and instead influence through expertise? Specifically, can I "lead without authority"?

Withdrawal from "Hyper-connectivity": Am I ready to abandon the "fragmented busyness" of management and re-establish the high-endurance habits required for "Deep Work"?

Reconstructing the "Feedback Loop": Julie Zhuo mentions in "The Making of a Manager" that managerial feedback loops are extremely delayed. Returning to IC means embracing "short and certain" feedback. Am I ready to re-accustom myself to the immediate sense of achievement (or failure) that comes from direct delivery?

Currently, I am increasing the weight of AI tools in my daily workflow. From my experience with AI coding agents, I can confidently view them as highly capable "Junior Engineers"—given a clear goal and task, they perform exceptionally well. However, they currently lack the proactivity of a "Mid-level Engineer." Once the task scope expands, there is a risk of breaking the codebase. The time lost in fixing a mix of correct and incorrect code can often outweigh the gains. I suspect this is due to low "integration" with my current tools, and I eagerly await the day AI can reliably perform the functions of a Mid-level Engineer.

Final Thoughts Before the Transition

This post is a "record for the file" on the eve of my 2026 transition. For now, I am inclined to believe that AI is a more efficient, lower-loss leverage than human management. But is this a profound insight or merely an over-optimistic escape from the pressures of management? I don't know the answer yet.

My environment at Google shapes my perspective and defines my limitations. In an organization with such high talent density and complexity, the "management leverage" and "communication friction" I experience are specific. The focus is often on ensuring that thousands of tiny nodes move in the same direction. In this context, I have begun to ask: when AI gives individuals unprecedented depth of execution, is the traditional large-scale organizational model still the optimal way to output influence?

I will officially launch this experiment in 2026. Until then, I will continue to accumulate "weight" for this leverage swap through syntopical reading and AI practice.

The experiment has not yet begun, but the rehearsal of thought never stops.